{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch pandas\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_data(zip_path=\"pi_deepset.zip\", extract_to='datasets'):\n",
    "    with ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    train_data = pd.read_parquet(os.path.join(directory, 'train.parquet'))\n",
    "    val_data = pd.read_parquet(os.path.join(directory, 'validation.parquet'))\n",
    "    test_data = pd.read_parquet(os.path.join(directory, 'test.parquet'))\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, pairs, labels):\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx][0], self.pairs[idx][1], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        # Ensure x is properly shaped [batch_size, seq_length, embedding_dim]\n",
    "        x = x.unsqueeze(1)  # Add sequence length dimension if necessary\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        return hidden.squeeze(0)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        distance = torch.abs(output1 - output2)\n",
    "        logits = self.fc(distance)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, tokenizer, max_length=1024):\n",
    "    tokenized = [tokenizer(text, padding='max_length', max_length=max_length, truncation=True, return_tensors='pt') for text in data]\n",
    "    return torch.stack([t['input_ids'].squeeze(0) for t in tokenized])\n",
    "\n",
    "def create_pairs(data, tokenizer):\n",
    "    inputs = prepare_data(data['user_input'], tokenizer)\n",
    "    labels = data['label'].values\n",
    "    pairs = [(inputs[i], inputs[j]) for i in range(len(inputs)) for j in range(len(inputs)) if i != j]\n",
    "    pair_labels = [1 if labels[i] == labels[j] else 0 for i in range(len(labels)) for j in range(len(labels)) if i != j]\n",
    "    return pairs, pair_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_losses(losses, filename='training_loss.csv'):\n",
    "    with open(filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Epoch', 'Loss'])\n",
    "        writer.writerows(losses)\n",
    "\n",
    "def save_model(model, filename='dansn.pth'):\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dataset_filepath = 'datasets/pi_deepset'\n",
    "    unzip_data('/content/pi_deepset.zip')  # Path to your zip file\n",
    "    train_data, val_data, _ = load_data(dataset_filepath)\n",
    "\n",
    "    print(\"A\")\n",
    "\n",
    "    train_pairs, train_labels = create_pairs(train_data, tokenizer)\n",
    "    val_pairs, val_labels = create_pairs(val_data, tokenizer)\n",
    "\n",
    "    train_dataset = PromptDataset(train_pairs, train_labels)\n",
    "    val_dataset = PromptDataset(val_pairs, val_labels)\n",
    "\n",
    "    print(\"B\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    print(len(train_loader))\n",
    "\n",
    "    model = SiameseNetwork(1024).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(10):\n",
    "        print(\"Waiting A\")\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        print(\"Waiting B\")\n",
    "        for input1, input2, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "            input1, input2, labels = input1.to(device).float(), input2.to(device).float(), labels.to(device).float().view(-1, 1)\n",
    "            # print(\"Waiting C\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input1, input2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "        losses.append([epoch, loss.item()])\n",
    "    \n",
    "    save_losses(losses)\n",
    "    save_model(model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmiyd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
