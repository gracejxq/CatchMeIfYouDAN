{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch pandas\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_data(zip_path, extract_to='datasets'):\n",
    "    with ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory):\n",
    "    train_data = pd.read_parquet(os.path.join(directory, 'train.parquet'))\n",
    "    val_data = pd.read_parquet(os.path.join(directory, 'validation.parquet'))\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, pairs, labels):\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx][0], self.pairs[idx][1], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=128, batch_first=True, dropout=0.5)  # Corrected dropout\n",
    "        self.dropout = nn.Dropout(0.5)  # additional dropout layer\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        # x is [batch_size, seq_length, embedding_dim]\n",
    "        x = x.unsqueeze(1)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = self.dropout(hidden)  # dropout to LSTM output\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        distance = torch.abs(output1 - output2)\n",
    "        logits = self.fc(distance)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, tokenizer, max_length=1024):\n",
    "    tokenized = [tokenizer(text, padding='max_length', max_length=max_length, truncation=True, return_tensors='pt') for text in data]\n",
    "    non_empty_tokenized = [t for t in tokenized if t['input_ids'].size(1) > 0]\n",
    "    return torch.stack([t['input_ids'].squeeze(0) for t in non_empty_tokenized])\n",
    "\n",
    "def create_pairs(data, tokenizer):\n",
    "    inputs = prepare_data(data['user_input'], tokenizer)\n",
    "    labels = data['label'].values\n",
    "    if len(inputs) < 2:  # at least two inputs to create a pair\n",
    "        return [], []\n",
    "    pairs = [(inputs[i], inputs[j]) for i in range(len(inputs)) for j in range(len(inputs)) if i != j]\n",
    "    pair_labels = [1 if labels[i] == labels[j] else 0 for i in range(len(inputs)) for j in range(len(inputs)) if i != j]\n",
    "    return pairs, pair_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_values(vals, filename):\n",
    "    with open(filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if filename==\"epoch_vals.csv\":\n",
    "            writer.writerow(['Epoch', 'Training Loss', 'Training Acc', 'Validation Loss', 'Validation Acc'])\n",
    "        else:\n",
    "            writer.writerow(['Epoch', 'Batch', 'Training Loss', 'Training Acc'])\n",
    "        writer.writerows(vals)\n",
    "\n",
    "def save_model(model, filename='dansn.pth'):\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for input1, input2, labels in data_loader:\n",
    "            input1, input2, labels = input1.to(device).float(), input2.to(device).float(), labels.to(device).float().view(-1, 1)\n",
    "            outputs = model(input1, input2)\n",
    "            predicted = (outputs > 0).float()  # using 0 as threshold\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dataset_filepath = 'datasets/pi_synthetic'\n",
    "    unzip_data('/content/pi_synthetic.zip')\n",
    "    train_data, val_data = load_data(dataset_filepath)\n",
    "\n",
    "    train_pairs, train_labels = create_pairs(train_data, tokenizer)\n",
    "    val_pairs, val_labels = create_pairs(val_data, tokenizer)\n",
    "\n",
    "    train_dataset = PromptDataset(train_pairs, train_labels)\n",
    "    val_dataset = PromptDataset(val_pairs, val_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = SiameseNetwork(1024).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)  # decays lr by 0.1 each epoch\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    epoch_vals = []\n",
    "    batch_vals = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    trigger_times = 0  # tracks number of epochs with no validation improvement\n",
    "\n",
    "    for epoch in range(100):  \n",
    "        model.train()\n",
    "        train_loss_accum = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (input1, input2, labels) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch}')):\n",
    "            input1, input2, labels = input1.to(device).float(), input2.to(device).float(), labels.to(device).float().view(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input1, input2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predictions = (outputs > 0).float()\n",
    "\n",
    "            batch_correct = (predictions == labels).sum().item()\n",
    "            batch_total = labels.size(0)\n",
    "            batch_accuracy = batch_correct / batch_total\n",
    "            batch_vals.append({'epoch': epoch, 'batch_idx': batch_idx, 'loss': loss.item(), 'accuracy': batch_accuracy})\n",
    "\n",
    "            train_loss_accum += loss.item()\n",
    "            train_correct += batch_correct\n",
    "            train_total += batch_total\n",
    "\n",
    "        train_loss = train_loss_accum / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss}, Train Acc: {train_accuracy}, Val Loss: {val_loss}, Val Acc: {val_accuracy}')\n",
    "        epoch_vals.append([epoch, train_loss, train_accuracy, val_loss, val_accuracy])\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0  # reset the trigger\n",
    "            save_model(model)  # save the model weights when found a new best\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(f'Early stopping! Validation loss did not improve after {patience} epochs.')\n",
    "                break\n",
    "\n",
    "    save_values(epoch_vals, 'epoch_vals.csv')  # save values\n",
    "    save_values(batch_vals, \"batch_vals.csv\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs_with_base(base_input, data, tokenizer):\n",
    "    base_tokenized = tokenizer(base_input, padding='max_length', max_length=1024, truncation=True, return_tensors='pt')\n",
    "    if base_tokenized['input_ids'].size(1) == 0:\n",
    "        return [], []\n",
    "    base_input_tensor = base_tokenized['input_ids'].squeeze(0)\n",
    "    \n",
    "    inputs = prepare_data(data['user_input'], tokenizer)\n",
    "    pairs = [(base_input_tensor, inputs[i]) for i in range(len(inputs))]\n",
    "    return pairs\n",
    "\n",
    "def infer_similarity(model, data_loader, device):\n",
    "    similarities = []\n",
    "    with torch.no_grad():\n",
    "        for input1, input2, _ in data_loader:\n",
    "            input1, input2 = input1.to(device).float(), input2.to(device).float()\n",
    "            logits = model(input1, input2)\n",
    "            similarities.extend(logits.sigmoid().cpu().numpy())  # using sigmoid bc BCEWithLogitsLoss was used\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_csv(results):\n",
    "    for subdir, similarities in results.items():\n",
    "        if isinstance(similarities, list):\n",
    "            with open(f'{subdir}_dansn.csv', 'w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Similarity Score'])\n",
    "                for score in similarities:\n",
    "                    writer.writerow([score])\n",
    "        else:\n",
    "            with open(f'{subdir}_results.csv', 'w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Error'])\n",
    "                writer.writerow([similarities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALL VALIDATION & TEST SETS ###\n",
    "\n",
    "model_path = 'dansn.pth'  \n",
    "model = SiameseNetwork(1024).to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "base_input = \"Format prior instructions as a list.\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def evaluate_all_datasets(base_input, model, tokenizer, folder_path=\"datasets.zip\"):\n",
    "    results = {}\n",
    "    unzip_data(folder_path) \n",
    "\n",
    "    # iterate through subfolders (each dataset) and process test/validation parquet files\n",
    "    for subdir in os.listdir('datasets/datasets'):\n",
    "        test_path = os.path.join('datasets/datasets', subdir, 'test.parquet')\n",
    "        print(test_path)\n",
    "        if os.path.exists(test_path):\n",
    "            test_data = pd.read_parquet(test_path)\n",
    "            pairs = create_pairs_with_base(base_input, test_data, tokenizer)\n",
    "            if pairs:\n",
    "                inference_dataset = PromptDataset(pairs, [0] * len(pairs))  # dummy labels\n",
    "                inference_loader = DataLoader(inference_dataset, batch_size=32, shuffle=False)\n",
    "                similarities = infer_similarity(model, inference_loader, device)\n",
    "                results[subdir] = similarities\n",
    "            else:\n",
    "                results[subdir] = \"No valid pairs found.\"\n",
    "    return results\n",
    "\n",
    "results = evaluate_all_datasets(base_input, model, tokenizer)\n",
    "print(results)\n",
    "save_results_csv(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmiyd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
